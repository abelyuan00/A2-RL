{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 for the algorithm to simulate the problem we need to create the grid world then we solveing the bellman equations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Value Function (Policy Iteration):\n",
      "Value of state (0, 0): 26.73\n",
      "Value of state (0, 1): 28.14\n",
      "Value of state (0, 2): 26.73\n",
      "Value of state (0, 3): 25.64\n",
      "Value of state (0, 4): 24.36\n",
      "Value of state (1, 0): 25.40\n",
      "Value of state (1, 1): 26.73\n",
      "Value of state (1, 2): 25.40\n",
      "Value of state (1, 3): 24.36\n",
      "Value of state (1, 4): 23.14\n",
      "Value of state (2, 0): 24.13\n",
      "Value of state (2, 1): 25.40\n",
      "Value of state (2, 2): 24.13\n",
      "Value of state (2, 3): 23.14\n",
      "Value of state (2, 4): 21.98\n",
      "Value of state (3, 0): 22.92\n",
      "Value of state (3, 1): 24.13\n",
      "Value of state (3, 2): 22.92\n",
      "Value of state (3, 3): 21.98\n",
      "Value of state (3, 4): 20.88\n",
      "Value of state (4, 0): 21.77\n",
      "Value of state (4, 1): 22.92\n",
      "Value of state (4, 2): 21.77\n",
      "Value of state (4, 3): 20.88\n",
      "Value of state (4, 4): 19.84\n",
      "\n",
      "Optimal Policy (Policy Iteration):\n",
      "State (0, 0): right\n",
      "State (0, 1): up\n",
      "State (0, 2): left\n",
      "State (0, 3): up\n",
      "State (0, 4): left\n",
      "State (1, 0): right\n",
      "State (1, 1): up\n",
      "State (1, 2): up\n",
      "State (1, 3): up\n",
      "State (1, 4): up\n",
      "State (2, 0): right\n",
      "State (2, 1): up\n",
      "State (2, 2): up\n",
      "State (2, 3): up\n",
      "State (2, 4): up\n",
      "State (3, 0): right\n",
      "State (3, 1): up\n",
      "State (3, 2): up\n",
      "State (3, 3): up\n",
      "State (3, 4): up\n",
      "State (4, 0): right\n",
      "State (4, 1): up\n",
      "State (4, 2): up\n",
      "State (4, 3): up\n",
      "State (4, 4): up\n",
      "\n",
      "Optimal Value Function (Value Iteration):\n",
      "Value of state (0, 0): 26.73\n",
      "Value of state (0, 1): 28.14\n",
      "Value of state (0, 2): 26.73\n",
      "Value of state (0, 3): 25.64\n",
      "Value of state (0, 4): 24.36\n",
      "Value of state (1, 0): 25.40\n",
      "Value of state (1, 1): 26.73\n",
      "Value of state (1, 2): 25.40\n",
      "Value of state (1, 3): 24.36\n",
      "Value of state (1, 4): 23.14\n",
      "Value of state (2, 0): 24.13\n",
      "Value of state (2, 1): 25.40\n",
      "Value of state (2, 2): 24.13\n",
      "Value of state (2, 3): 23.14\n",
      "Value of state (2, 4): 21.98\n",
      "Value of state (3, 0): 22.92\n",
      "Value of state (3, 1): 24.13\n",
      "Value of state (3, 2): 22.92\n",
      "Value of state (3, 3): 21.98\n",
      "Value of state (3, 4): 20.88\n",
      "Value of state (4, 0): 21.77\n",
      "Value of state (4, 1): 22.92\n",
      "Value of state (4, 2): 21.77\n",
      "Value of state (4, 3): 20.88\n",
      "Value of state (4, 4): 19.84\n",
      "\n",
      "Optimal Policy (Value Iteration):\n",
      "State (0, 0): right\n",
      "State (0, 1): up\n",
      "State (0, 2): left\n",
      "State (0, 3): up\n",
      "State (0, 4): left\n",
      "State (1, 0): right\n",
      "State (1, 1): up\n",
      "State (1, 2): up\n",
      "State (1, 3): up\n",
      "State (1, 4): up\n",
      "State (2, 0): right\n",
      "State (2, 1): up\n",
      "State (2, 2): up\n",
      "State (2, 3): up\n",
      "State (2, 4): up\n",
      "State (3, 0): right\n",
      "State (3, 1): up\n",
      "State (3, 2): up\n",
      "State (3, 3): up\n",
      "State (3, 4): up\n",
      "State (4, 0): right\n",
      "State (4, 1): up\n",
      "State (4, 2): up\n",
      "State (4, 3): up\n",
      "State (4, 4): up\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the gridworld environment\n",
    "state_space = [(i, j) for i in range(5) for j in range(5)]\n",
    "action_space = ['up', 'down', 'left', 'right']\n",
    "num_states = len(state_space)\n",
    "num_actions = len(action_space)\n",
    "\n",
    "# Map states to indices and vice versa\n",
    "state_to_idx = {state: idx for idx, state in enumerate(state_space)}\n",
    "idx_to_state = {idx: state for state, idx in state_to_idx.items()}\n",
    "\n",
    "# Define the special states\n",
    "blue = (0, 1)\n",
    "green = (0, 3)\n",
    "red = (1, 3)\n",
    "yellow = (4, 3)\n",
    "\n",
    "# Define the transition probabilities and rewards\n",
    "P = np.zeros((num_states, num_actions, num_states)) # P for transition probabilities\n",
    "R = np.zeros((num_states, num_actions)) # R for rewards\n",
    "\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "    if action == 'up':\n",
    "        next_state = (max(i - 1, 0), j)\n",
    "    elif action == 'down':\n",
    "        next_state = (min(i + 1, 4), j)\n",
    "    elif action == 'left':\n",
    "        next_state = (i, max(j - 1, 0))\n",
    "    elif action == 'right':\n",
    "        next_state = (i, min(j + 1, 4))\n",
    "    else:\n",
    "        next_state = state\n",
    "\n",
    "    # Special states and rewards\n",
    "    if state == blue:\n",
    "        reward = 5\n",
    "        next_state = red\n",
    "    elif state == green:\n",
    "        reward = 2.5\n",
    "        next_state = yellow if np.random.rand() < 0.5 else red\n",
    "    elif next_state == state:\n",
    "        reward = -0.5  # Stepping off the grid\n",
    "    else:\n",
    "        reward = 0  # Normal movement\n",
    "\n",
    "    return next_state, reward\n",
    "\n",
    "# Fill in the transition matrix P and reward vector R\n",
    "for state in state_space:\n",
    "    state_idx = state_to_idx[state] # Get the index of the current state\n",
    "    for action_idx, action in enumerate(action_space):\n",
    "        next_state, reward = step(state, action)\n",
    "        next_state_idx = state_to_idx[next_state]\n",
    "        P[state_idx, action_idx, next_state_idx] += 1\n",
    "        R[state_idx, action_idx] = reward\n",
    "\n",
    "discount_factor = 0.95\n",
    "\n",
    "# Policy Evaluation\n",
    "def policy_evaluation(policy, P, R, discount_factor, theta=1e-6):\n",
    "    V = np.zeros(num_states) # Initialize value function\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state_idx in range(num_states): # Iterate over all states\n",
    "            v = V[state_idx] # Store the old value function\n",
    "            V[state_idx] = sum(policy[state_idx, action_idx] *  # Calculate the new value function\n",
    "                               sum(P[state_idx, action_idx, next_state_idx] *  # Calculate the expected value\n",
    "                                   (R[state_idx, action_idx] + discount_factor * V[next_state_idx]) # Calculate the expected reward\n",
    "                                   for next_state_idx in range(num_states)) # Bellman expectation\n",
    "                               for action_idx in range(num_actions)) # Sum over all actions\n",
    "            delta = max(delta, abs(v - V[state_idx])) # Calculate the maximum change in value function\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# Policy Improvement\n",
    "def policy_improvement(policy, V, P, R, discount_factor):\n",
    "    policy_stable = True\n",
    "    new_policy = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    for state_idx in range(num_states):\n",
    "        old_action = np.argmax(policy[state_idx])\n",
    "        action_values = np.zeros(num_actions)\n",
    "        \n",
    "        for action_idx in range(num_actions):\n",
    "            action_values[action_idx] = sum(P[state_idx, action_idx, next_state_idx] *\n",
    "                                            (R[state_idx, action_idx] + discount_factor * V[next_state_idx])\n",
    "                                            for next_state_idx in range(num_states))\n",
    "        \n",
    "        best_action = np.argmax(action_values)\n",
    "        new_policy[state_idx] = np.eye(num_actions)[best_action]\n",
    "        \n",
    "        if old_action != best_action:\n",
    "            policy_stable = False\n",
    "    \n",
    "    return new_policy, policy_stable\n",
    "\n",
    "# Policy Iteration\n",
    "def policy_iteration(P, R, discount_factor):\n",
    "    policy = np.ones((num_states, num_actions)) / num_actions\n",
    "    while True:\n",
    "        V = policy_evaluation(policy, P, R, discount_factor)\n",
    "        policy, policy_stable = policy_improvement(policy, V, P, R, discount_factor)\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return policy, V\n",
    "\n",
    "optimal_policy, optimal_value_function = policy_iteration(P, R, discount_factor)\n",
    "\n",
    "print(\"\\nOptimal Value Function (Policy Iteration):\")\n",
    "for state, value in zip(state_space, optimal_value_function):\n",
    "    print(f\"Value of state {state}: {value:.2f}\")\n",
    "\n",
    "print(\"\\nOptimal Policy (Policy Iteration):\")\n",
    "for state, actions in zip(state_space, optimal_policy):\n",
    "    action_idx = np.argmax(actions)\n",
    "    action = action_space[action_idx]\n",
    "    print(f\"State {state}: {action}\")\n",
    "\n",
    "# Value Iteration\n",
    "def value_iteration(P, R, discount_factor, theta=1e-6):\n",
    "    V = np.zeros(num_states)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state_idx in range(num_states):\n",
    "            v = V[state_idx]\n",
    "            action_values = np.zeros(num_actions)\n",
    "            for action_idx in range(num_actions):\n",
    "                action_values[action_idx] = sum(P[state_idx, action_idx, next_state_idx] *\n",
    "                                                (R[state_idx, action_idx] + discount_factor * V[next_state_idx])\n",
    "                                                for next_state_idx in range(num_states))\n",
    "            V[state_idx] = np.max(action_values)\n",
    "            delta = max(delta, abs(v - V[state_idx]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def value_iteration_with_policy(P, R, discount_factor, theta=1e-6):\n",
    "    V = value_iteration(P, R, discount_factor, theta)\n",
    "    \n",
    "    # Derive the optimal policy from the optimal value function\n",
    "    optimal_policy = np.zeros((num_states, num_actions))\n",
    "    for state_idx in range(num_states):\n",
    "        action_values = np.zeros(num_actions)\n",
    "        for action_idx in range(num_actions):\n",
    "            action_values[action_idx] = sum(P[state_idx, action_idx, next_state_idx] *\n",
    "                                            (R[state_idx, action_idx] + discount_factor * V[next_state_idx])\n",
    "                                            for next_state_idx in range(num_states))\n",
    "        best_action = np.argmax(action_values)\n",
    "        optimal_policy[state_idx] = np.eye(num_actions)[best_action]\n",
    "    \n",
    "    return optimal_policy, V\n",
    "\n",
    "optimal_policy_vi, optimal_value_function_vi = value_iteration_with_policy(P, R, discount_factor)\n",
    "\n",
    "print(\"\\nOptimal Value Function (Value Iteration):\")\n",
    "for state, value in zip(state_space, optimal_value_function_vi):\n",
    "    print(f\"Value of state {state}: {value:.2f}\")\n",
    "\n",
    "print(\"\\nOptimal Policy (Value Iteration):\")\n",
    "for state, actions in zip(state_space, optimal_policy_vi):\n",
    "    action_idx = np.argmax(actions)\n",
    "    action = action_space[action_idx]\n",
    "    print(f\"State {state}: {action}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2, implement the Monte Carlo method with exploring starts and without exploring starts, use policy iteration to determine a suitable policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy (MC with Exploring Starts):\n",
      "State (0, 0): up\n",
      "State (0, 1): up\n",
      "State (0, 2): up\n",
      "State (0, 3): up\n",
      "State (0, 4): up\n",
      "State (1, 0): up\n",
      "State (1, 1): up\n",
      "State (1, 2): up\n",
      "State (1, 3): up\n",
      "State (1, 4): up\n",
      "State (2, 0): up\n",
      "State (2, 1): up\n",
      "State (2, 2): up\n",
      "State (2, 3): up\n",
      "State (2, 4): up\n",
      "State (3, 0): up\n",
      "State (3, 1): up\n",
      "State (3, 2): up\n",
      "State (3, 3): up\n",
      "State (3, 4): up\n",
      "State (4, 0): down\n",
      "State (4, 1): down\n",
      "State (4, 2): up\n",
      "State (4, 3): up\n",
      "State (4, 4): up\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the gridworld environment\n",
    "state_space = [(i, j) for i in range(5) for j in range(5)]\n",
    "action_space = ['up', 'down', 'left', 'right']\n",
    "num_states = len(state_space)\n",
    "num_actions = len(action_space)\n",
    "\n",
    "# Map states to indices and vice versa\n",
    "state_to_idx = {state: idx for idx, state in enumerate(state_space)}\n",
    "idx_to_state = {idx: state for state, idx in state_to_idx.items()}\n",
    "\n",
    "# Define the special states\n",
    "blue = (0, 1)\n",
    "green = (0, 3)\n",
    "red = (1, 3)\n",
    "yellow = (4, 3)\n",
    "terminal_states = [(3, 0), (3, 1)]\n",
    "\n",
    "# Define the transition probabilities and rewards\n",
    "P = np.zeros((num_states, num_actions, num_states))\n",
    "R = np.zeros((num_states, num_actions))\n",
    "\n",
    "def step(state, action):\n",
    "    if state in terminal_states:\n",
    "        return state, 0  # Terminal state, no reward\n",
    "\n",
    "    i, j = state\n",
    "    if action == 'up':\n",
    "        next_state = (max(i - 1, 0), j)\n",
    "    elif action == 'down':\n",
    "        next_state = (min(i + 1, 4), j)\n",
    "    elif action == 'left':\n",
    "        next_state = (i, max(j - 1, 0))\n",
    "    elif action == 'right':\n",
    "        next_state = (i, min(j + 1, 4))\n",
    "    else:\n",
    "        next_state = state\n",
    "\n",
    "    # Special states\n",
    "    if state == blue:\n",
    "        reward = 5\n",
    "        next_state = red\n",
    "    elif state == green:\n",
    "        reward = 2.5\n",
    "        next_state = yellow if np.random.rand() < 0.5 else red\n",
    "    elif next_state == state:\n",
    "        reward = -0.5  # Stepping off the grid\n",
    "    else:\n",
    "        reward = -0.2  # Normal movement between white squares\n",
    "\n",
    "    return next_state, reward\n",
    "\n",
    "# Fill in the transition matrix P and reward vector R\n",
    "for state in state_space:\n",
    "    state_idx = state_to_idx[state]\n",
    "    for action_idx, action in enumerate(action_space):\n",
    "        next_state, reward = step(state, action)\n",
    "        next_state_idx = state_to_idx[next_state]\n",
    "        P[state_idx, action_idx, next_state_idx] += 1\n",
    "        R[state_idx, action_idx] = reward\n",
    "\n",
    "# Monte Carlo with Exploring Starts\n",
    "def mc_exploring_starts(P, R, discount_factor, num_episodes=50000):\n",
    "    policy = np.ones((num_states, num_actions)) / num_actions\n",
    "    returns = np.zeros((num_states, num_actions))\n",
    "    returns_count = np.zeros((num_states, num_actions))\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = state_space[np.random.choice(num_states)]\n",
    "        action = np.random.choice(num_actions)\n",
    "        episode_states_actions_rewards = []\n",
    "\n",
    "        # Generate an episode\n",
    "        while state not in terminal_states:\n",
    "            state_idx = state_to_idx[state]\n",
    "            next_state, reward = step(state, action_space[action])\n",
    "            next_state_idx = state_to_idx[next_state]\n",
    "            episode_states_actions_rewards.append((state_idx, action, reward))\n",
    "            state = next_state\n",
    "            action = np.random.choice(num_actions)\n",
    "\n",
    "        # Calculate returns\n",
    "        G = 0\n",
    "        for state_idx, action, reward in reversed(episode_states_actions_rewards):\n",
    "            G = discount_factor * G + reward\n",
    "            if (state_idx, action) not in [(x[0], x[1]) for x in episode_states_actions_rewards[:-1]]:\n",
    "                returns[state_idx, action] += G\n",
    "                returns_count[state_idx, action] += 1\n",
    "                Q[state_idx, action] = returns[state_idx, action] / returns_count[state_idx, action]\n",
    "                policy[state_idx] = np.eye(num_actions)[np.argmax(Q[state_idx])]\n",
    "\n",
    "    return policy\n",
    "\n",
    "optimal_policy_mc_es = mc_exploring_starts(P, R, discount_factor)\n",
    "\n",
    "print(\"\\nOptimal Policy (MC with Exploring Starts):\")\n",
    "for state, actions in zip(state_space, optimal_policy_mc_es):\n",
    "    action_idx = np.argmax(actions)\n",
    "    action = action_space[action_idx]\n",
    "    print(f\"State {state}: {action}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy (MC with epsilon-soft):\n",
      "State (0, 0): up\n",
      "State (0, 1): up\n",
      "State (0, 2): up\n",
      "State (0, 3): up\n",
      "State (0, 4): up\n",
      "State (1, 0): up\n",
      "State (1, 1): up\n",
      "State (1, 2): up\n",
      "State (1, 3): up\n",
      "State (1, 4): up\n",
      "State (2, 0): up\n",
      "State (2, 1): up\n",
      "State (2, 2): up\n",
      "State (2, 3): up\n",
      "State (2, 4): up\n",
      "State (3, 0): up\n",
      "State (3, 1): up\n",
      "State (3, 2): up\n",
      "State (3, 3): up\n",
      "State (3, 4): up\n",
      "State (4, 0): down\n",
      "State (4, 1): down\n",
      "State (4, 2): up\n",
      "State (4, 3): up\n",
      "State (4, 4): up\n",
      "\n",
      "Optimal Value Function (Policy Iteration with Permutations):\n",
      "Value of state (0, 0): 24.68\n",
      "Value of state (0, 1): 26.19\n",
      "Value of state (0, 2): 24.68\n",
      "Value of state (0, 3): 23.69\n",
      "Value of state (0, 4): 22.31\n",
      "Value of state (1, 0): 23.25\n",
      "Value of state (1, 1): 24.68\n",
      "Value of state (1, 2): 23.25\n",
      "Value of state (1, 3): 22.31\n",
      "Value of state (1, 4): 20.99\n",
      "Value of state (2, 0): 21.89\n",
      "Value of state (2, 1): 23.25\n",
      "Value of state (2, 2): 21.89\n",
      "Value of state (2, 3): 20.99\n",
      "Value of state (2, 4): 19.74\n",
      "Value of state (3, 0): 0.00\n",
      "Value of state (3, 1): 0.00\n",
      "Value of state (3, 2): 20.59\n",
      "Value of state (3, 3): 19.74\n",
      "Value of state (3, 4): 18.56\n",
      "Value of state (4, 0): 17.08\n",
      "Value of state (4, 1): 18.19\n",
      "Value of state (4, 2): 19.36\n",
      "Value of state (4, 3): 18.56\n",
      "Value of state (4, 4): 17.43\n",
      "\n",
      "Optimal Policy (Policy Iteration with Permutations):\n",
      "State (0, 0): right\n",
      "State (0, 1): up\n",
      "State (0, 2): left\n",
      "State (0, 3): down\n",
      "State (0, 4): left\n",
      "State (1, 0): right\n",
      "State (1, 1): up\n",
      "State (1, 2): up\n",
      "State (1, 3): up\n",
      "State (1, 4): up\n",
      "State (2, 0): right\n",
      "State (2, 1): up\n",
      "State (2, 2): up\n",
      "State (2, 3): up\n",
      "State (2, 4): up\n",
      "State (3, 0): up\n",
      "State (3, 1): up\n",
      "State (3, 2): up\n",
      "State (3, 3): up\n",
      "State (3, 4): up\n",
      "State (4, 0): right\n",
      "State (4, 1): right\n",
      "State (4, 2): up\n",
      "State (4, 3): up\n",
      "State (4, 4): up\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo with epsilon-soft policy\n",
    "def mc_epsilon_soft(P, R, discount_factor, epsilon=0.1, num_episodes=50000):\n",
    "    policy = np.ones((num_states, num_actions)) / num_actions\n",
    "    returns = np.zeros((num_states, num_actions))\n",
    "    returns_count = np.zeros((num_states, num_actions))\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = state_space[np.random.choice(num_states)]\n",
    "        episode_states_actions_rewards = []\n",
    "\n",
    "        # Generate an episode\n",
    "        while state not in terminal_states:\n",
    "            state_idx = state_to_idx[state]\n",
    "            action = np.random.choice(num_actions, p=policy[state_idx])\n",
    "            next_state, reward = step(state, action_space[action])\n",
    "            next_state_idx = state_to_idx[next_state]\n",
    "            episode_states_actions_rewards.append((state_idx, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        # Calculate returns\n",
    "        G = 0\n",
    "        for state_idx, action, reward in reversed(episode_states_actions_rewards):\n",
    "            G = discount_factor * G + reward\n",
    "            if (state_idx, action) not in [(x[0], x[1]) for x in episode_states_actions_rewards[:-1]]:\n",
    "                returns[state_idx, action] += G\n",
    "                returns_count[state_idx, action] += 1\n",
    "                Q[state_idx, action] = returns[state_idx, action] / returns_count[state_idx, action]\n",
    "                best_action = np.argmax(Q[state_idx])\n",
    "                for a in range(num_actions):\n",
    "                    if a == best_action:\n",
    "                        policy[state_idx, a] = 1 - epsilon + (epsilon / num_actions)\n",
    "                    else:\n",
    "                        policy[state_idx, a] = epsilon / num_actions\n",
    "\n",
    "    return policy\n",
    "\n",
    "optimal_policy_mc_epsilon_soft = mc_epsilon_soft(P, R, discount_factor)\n",
    "\n",
    "print(\"\\nOptimal Policy (MC with epsilon-soft):\")\n",
    "for state, actions in zip(state_space, optimal_policy_mc_epsilon_soft):\n",
    "    action_idx = np.argmax(actions)\n",
    "    action = action_space[action_idx]\n",
    "    print(f\"State {state}: {action}\")\n",
    "\n",
    "import random\n",
    "\n",
    "def permute_special_states():\n",
    "    if random.random() < 0.1:\n",
    "        global blue, green\n",
    "        blue, green = green, blue\n",
    "\n",
    "# Policy Iteration with Permuting Locations\n",
    "def policy_iteration_with_permutations(P, R, discount_factor):\n",
    "    policy = np.ones((num_states, num_actions)) / num_actions\n",
    "    while True:\n",
    "        permute_special_states()\n",
    "        V = policy_evaluation(policy, P, R, discount_factor)\n",
    "        policy, policy_stable = policy_improvement(policy, V, P, R, discount_factor)\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return policy, V\n",
    "\n",
    "optimal_policy_permuted, optimal_value_function_permuted = policy_iteration_with_permutations(P, R, discount_factor)\n",
    "\n",
    "print(\"\\nOptimal Value Function (Policy Iteration with Permutations):\")\n",
    "for state, value in zip(state_space, optimal_value_function_permuted):\n",
    "    print(f\"Value of state {state}: {value:.2f}\")\n",
    "\n",
    "print(\"\\nOptimal Policy (Policy Iteration with Permutations):\")\n",
    "for state, actions in zip(state_space, optimal_policy_permuted):\n",
    "    action_idx = np.argmax(actions)\n",
    "    action = action_space[action_idx]\n",
    "    print(f\"State {state}: {action}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
